\documentclass[a4paper,10pt]{exam}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{latexsym}
\usepackage[colorlinks,linkcolor=red,anchorcolor=blue,citecolor=green]{hyperref}
\title{QUIZ3}
\date{}
\author{}
\printanswers
\begin{document}
	\maketitle
	\begin{questions}
		\question \textbf{Decision Tree}\\Impurity functions play an important role in decision tree branching. For binary classification problems, let $\mu_+$ be the fraction of positive examples in a data subset, and $\mu_- = 1 - \mu_+$ be the fraction of negative examples in the data subset.
		The Gini index is $1 - \mu_+^2 - \mu_-^2$. What is the maximum value of the Gini index among all $\mu_+ \in [0, 1]$?
		\begin{choices}
		\CorrectChoice 0.5
		\choice 0.75
		\choice 0.25
		\choice 0
		\choice 1\\
		\end{choices}
		
		\question Following Question 1, there are four possible impurity functions below. We can normalize each impurity function by dividing it with its maximum value among all $\mu_+ \in [0, 1]$ For instance, the classification error is simply $\min(\mu_+, \mu_-)$ and its maximum value is 0.5. So the normalized classification error is $2 \min(\mu_+, \mu_-)$. After normalization, which of the following impurity function is equivalent to the normalized Gini index?
		
		\begin{choices}
		  \CorrectChoice the squared regression error (used for branching in classification data sets), which is by definition $\mu_+ (1 - (\mu_+ - \mu_-))^2 + \mu_- (-1 - (\mu_+ - \mu_-))^2$.
		  \choice the entropy, which is $-\mu_+ \ln \mu_+ - \mu_- \ln \mu_-$, with $0 \log 0 \equiv 0$.
		  \choice the closeness, which is $1 - |\mu_+ - \mu_-|$.
		  \choice the classification error $\min(\mu_+, \mu_-)$.
		  \choice none of the other choices\\
		\end{choices}
		
		\question \textbf{Random Forest}\\
		If bootstrapping is used to sample $N' = pN$ examples out of $N$ examples and $N$ is very large. Approximately how many of the $N$ examples will not be sampled at all?
		\begin{choices}
			\choice $(1 - e^{-1/p}) \cdot N$
			\choice $(1 - e^{-p}) \cdot N$
			\choice $e^{-1} \cdot N$
			\choice $e^{-1/p} \cdot N$
			\CorrectChoice $e^{-p} \cdot N$\\
		\end{choices}
		
		\question Consider a Random Forest $G$ that consists of three binary classification trees $\{g_k\}_{k=1}^3$, where each tree is of test 0/1 error $E_{\text{out}}(g_1) = 0.1$, $E_{\text{out}}(g_2) = 0.2$, $E_{\text{out}}(g_3) = 0.3$. Which of the following is the exact possible range of $E_{\text{out}}(G)$?
		\begin{choices}
			\choice $0 \le E_{\text{out}}(G) \le 0.1$
			\choice $0.1 \le E_{\text{out}}(G) \le 0.6$
			\choice $0.2 \le E_{\text{out}}(G) \le 0.3$
			\choice $0.1 \le E_{\text{out}}(G) \le 0.3$
			\CorrectChoice $0.1 \le E_{\text{out}}(G) \le 0.3$\\
		\end{choices}
		
		\question Consider a Random Forest $G$ that consists of $K$ binary classification trees $\{g_k\}_{k=1}^K$, where $K$ is an odd integer. Each $g_k$ is of test 0/1 error $E_{\text{out}}(g_k) = e_k$. Which of the following is an upper bound of $E_{\text{out}}(G)$?
		\begin{choices}
			\CorrectChoice $\frac{2}{K+1} \sum_{k=1}^K e_k$
			\choice $\frac{1}{K} \sum_{k=1}^K e_k$
			\choice $\frac{1}{K+1} \sum_{k=1}^K e_k$
			\choice $\min_{1 \le k \le K} e_k$
			\choice $\max_{1 \le k \le K} e_k$\\
		\end{choices}
		
		\question \textbf{Gradient Boosting}\\
		Let $\epsilon_t$ be the weighted 0/1 error of each $g_t$ as described in the AdaBoost algorithm (Lecture 208), and $U_t = \sum_{n=1}^N u_n^{(t)}$ be the total example weight during AdaBoost. Which of the following equation expresses $U_{T+1}$ by $\epsilon_t$?
		\begin{choices}
			\choice none of the other choices
			\choice $\prod_{t=1}^T \epsilon_t$
			\choice $\sum_{t=1}^T (2 \sqrt{\epsilon_t(1-\epsilon_t)})$
			\choice $\sum_{t=1}^T \epsilon_t$
			\CorrectChoice $\prod_{t=1}^T (2 \sqrt{\epsilon_t(1-\epsilon_t)})$\\
		\end{choices}
		
		\question For the gradient boosted decision tree, if a tree with only one constant node is returned as $g_1$, and if $g_1(\mathbf{x}) = 2$, then after the first iteration, all $s_n$ is updated from $0$ to a new constant $\alpha_1 g_1(\mathbf{x}_n)$. What is $s_n$?
		
		\begin{choices}
			\choice 2
			\choice none of the other choices
			\choice $\max_{1 \le n \le N} y_n$
			\choice $\min_{1 \le n \le N} y_n$
			\CorrectChoice $\frac{1}{N} \sum_{n=1}^N y_n$\\
		\end{choices}
		
		\question For the gradient boosted decision tree, after updating all $s_n$ in iteration $t$ using the steepest $\eta$ as $\alpha_t$, what is the value of $\sum_{n=1}^N s_n g_t(\mathbf{x}_n)$?
		
		\begin{choices}
			\choice none of the other choices
			\CorrectChoice $\sum_{n=1}^N y_n g_t(\mathbf{x}_n)$
			\choice $\sum_{n=1}^N y_n^2$
			\choice $\sum_{n=1}^N y_n s_n$
			\choice 0
			
	    \end{choices}
	\end{questions}
\end{document}
